# ======================================
# BUILD VECTOR DATABASE FOR MULTI-CORPUS RAG
# ======================================

import os, json, glob, warnings
from pathlib import Path

# T·∫Øt to√†n b·ªô warning
warnings.filterwarnings("ignore")

# S·ª≠ d·ª•ng g√≥i m·ªõi nh·∫•t ch√≠nh th·ª©c c·ªßa LangChain
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

# ====== 1Ô∏è‚É£ C·∫§U H√åNH ======
CORPUS_DIR = Path("data_corpus")     # 4 folder tri th·ª©c: dsm5, icd11, mhgap, mhgap_ver2
DB_DIR     = Path("db")              # n∆°i l∆∞u vector database
DB_DIR.mkdir(exist_ok=True)

EMB_MODEL  = "BAAI/bge-m3"           # model embedding m·∫°nh, ƒëa ng√¥n ng·ªØ
os.environ["USE_TF"] = "0"           # t·∫Øt TensorFlow ƒë·ªÉ tr√°nh xung ƒë·ªôt Keras 3

# ====== 2Ô∏è‚É£ KH·ªûI T·∫†O EMBEDDING MODEL ======
print(f"üöÄ Loading embedding model: {EMB_MODEL}")
emb = HuggingFaceEmbeddings(
    model_name=EMB_MODEL,
    model_kwargs={"device": "cuda"},
    encode_kwargs={"batch_size": 2}  # ho·∫∑c 2 n·∫øu GPU nh·ªè
)
# ====== 3Ô∏è‚É£ T·∫†O VECTOR DATABASE RI√äNG CHO M·ªñI FOLDER ======
manifest = []
for corpus_dir in CORPUS_DIR.iterdir():
    if not corpus_dir.is_dir():
        continue

    corpus_name = corpus_dir.name
    print(f"\nüß† Building index for: {corpus_name}")

    docs = []
    for f in glob.glob(str(corpus_dir / "*.json")):
        try:
            item = json.load(open(f, encoding="utf-8"))
        except Exception as e:
            print(f"‚ö†Ô∏è  Error reading {f}: {e}")
            continue

        text = item.get("index_text") or item.get("text_en") or ""
        if not text.strip():
            continue

        # --- Chuy·ªÉn list/dict trong metadata th√†nh chu·ªói ---
        def safe(v):
            if isinstance(v, list):
                return "; ".join(map(str, v))
            if isinstance(v, (dict, set)):
                return str(v)
            return v

        metadata = {
            "id": safe(item.get("id", "")),
            "title_en": safe(item.get("title_en", "")),
            "source": safe(item.get("source", corpus_name)),
            "condition": safe(item.get("axes", {}).get("condition", [])),
            "type": safe(item.get("axes", {}).get("type", [])),
            "clinical_section": safe(item.get("axes", {}).get("clinical_section", [])),
            "advice_section": safe(item.get("axes", {}).get("advice_section", [])),
            "risk_band": safe(item.get("axes", {}).get("risk_band", "low")),
        }

        docs.append(Document(page_content=text, metadata=metadata))

    if not docs:
        print(f"‚ö†Ô∏è  No valid documents found in {corpus_name}, skipping.")
        continue

    out_dir = DB_DIR / corpus_name
    out_dir.mkdir(exist_ok=True)

    # --- T·∫°o index ---
    db = Chroma.from_documents(
        docs,
        embedding=emb,
        persist_directory=str(out_dir)
    )
    db.persist()

    manifest.append({
        "corpus": corpus_name,
        "num_snippets": len(docs),
        "db_path": str(out_dir)
    })
    print(f"‚úÖ Indexed {len(docs)} snippets ‚Üí {out_dir}")

# ====== 4Ô∏è‚É£ L∆ØU MANIFEST ======
manifest_path = DB_DIR / "_manifest.json"
manifest_path.write_text(
    json.dumps(manifest, indent=2, ensure_ascii=False),
    encoding="utf-8"
)

print("\nüéØ All vector databases built successfully!")
print(f"üìÑ Manifest saved at: {manifest_path}")
