# ======================================
# BUILD VECTOR DATABASE FOR MULTI-CORPUS RAG
# ======================================

import os, json, glob, warnings
from pathlib import Path

# T·∫Øt to√†n b·ªô warning
warnings.filterwarnings("ignore")

# S·ª≠ d·ª•ng g√≥i m·ªõi nh·∫•t ch√≠nh th·ª©c c·ªßa LangChain
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

# ====== 1Ô∏è‚É£ C·∫§U H√åNH ======
SCRIPT_DIR = Path(__file__).parent.resolve()
CORPUS_DIR = SCRIPT_DIR / "../../data/reasoning"     # 4 folder tri th·ª©c: dsm5, icd11, mhgap, mhgap_ver2
DB_DIR     = SCRIPT_DIR / "../../vector_db/Deep_Reasoning"              # n∆°i l∆∞u vector database
DB_DIR.mkdir(parents=True, exist_ok=True)

EMB_MODEL  = "BAAI/bge-m3"           # model embedding m·∫°nh, ƒëa ng√¥n ng·ªØ
os.environ["USE_TF"] = "0"           # t·∫Øt TensorFlow ƒë·ªÉ tr√°nh xung ƒë·ªôt Keras 3

# ====== 2Ô∏è‚É£ KH·ªûI T·∫†O EMBEDDING MODEL ======
print(f"üöÄ Loading embedding model: {EMB_MODEL}")
emb = HuggingFaceEmbeddings(
    model_name=EMB_MODEL,
    model_kwargs={"device": "cuda"},
    encode_kwargs={"batch_size": 1}  # Reduced to 1 to prevent OOM
)
# ====== 3Ô∏è‚É£ T·∫†O VECTOR DATABASE RI√äNG CHO M·ªñI FOLDER ======
manifest = []
for corpus_dir in CORPUS_DIR.iterdir():
    if not corpus_dir.is_dir():
        continue

    corpus_name = corpus_dir.name
    print(f"\nüß† Building index for: {corpus_name}")

    docs = []
    for f in glob.glob(str(corpus_dir / "*.json")):
        try:
            item = json.load(open(f, encoding="utf-8"))
        except Exception as e:
            print(f"‚ö†Ô∏è  Error reading {f}: {e}")
            continue

        text = item.get("index_text") or item.get("text_en") or item.get("text") or ""
        if not text.strip():
            continue

        # --- Chuy·ªÉn list/dict trong metadata th√†nh chu·ªói ---
        def safe(v):
            if isinstance(v, list):
                return "; ".join(map(str, v))
            if isinstance(v, (dict, set)):
                return str(v)
            return v

        metadata = {
            "id": safe(item.get("id", "")),
            "corpus": safe(item.get("corpus", corpus_name)),
            "condition": safe(item.get("condition", "")),
            "source": safe(item.get("source", corpus_name)),
        }

        docs.append(Document(page_content=text, metadata=metadata))

    if not docs:
        print(f"‚ö†Ô∏è  No valid documents found in {corpus_name}, skipping.")
        continue

    out_dir = DB_DIR / corpus_name
    out_dir.mkdir(exist_ok=True)

    # --- T·∫°o index v·ªõi batch processing ƒë·ªÉ tr√°nh OOM ---
    print(f"   Processing {len(docs)} documents in batches...")
    batch_size = 50  # Process 50 documents at a time
    
    if len(docs) <= batch_size:
        db = Chroma.from_documents(
            docs,
            embedding=emb,
            persist_directory=str(out_dir)
        )
    else:
        # First batch creates the database
        db = Chroma.from_documents(
            docs[:batch_size],
            embedding=emb,
            persist_directory=str(out_dir)
        )
        # Add remaining documents in batches
        for i in range(batch_size, len(docs), batch_size):
            batch = docs[i:i+batch_size]
            db.add_documents(batch)
            print(f"   Processed {min(i+batch_size, len(docs))}/{len(docs)} documents")
    
    db.persist()

    manifest.append({
        "corpus": corpus_name,
        "num_snippets": len(docs),
        "db_path": str(out_dir)
    })
    print(f"‚úÖ Indexed {len(docs)} snippets ‚Üí {out_dir}")

# ====== 4Ô∏è‚É£ L∆ØU MANIFEST ======
manifest_path = DB_DIR / "_manifest.json"
manifest_path.write_text(
    json.dumps(manifest, indent=2, ensure_ascii=False),
    encoding="utf-8"
)

print("\nüéØ All vector databases built successfully!")
print(f"üìÑ Manifest saved at: {manifest_path}")
