{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10730648,"sourceType":"datasetVersion","datasetId":6525542}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # # This Python 3 environment comes with many helpful analytics libraries installed\n# # # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # # For example, here's several helpful packages to load\n\n# # import numpy as np # linear algebra\n# # import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # # Input data files are available in the read-only \"../input/\" directory\n# # # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input/cache-meld'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T09:03:53.381908Z","iopub.execute_input":"2025-11-18T09:03:53.382220Z","iopub.status.idle":"2025-11-18T09:03:53.386586Z","shell.execute_reply.started":"2025-11-18T09:03:53.382183Z","shell.execute_reply":"2025-11-18T09:03:53.386026Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# --- CELL 1: C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN (B·∫¢N ·ªîN ƒê·ªäNH) ---\n\n# 1. G·ª° c√†i ƒë·∫∑t phi√™n b·∫£n l·ªói hi·ªán t·∫°i\n!pip uninstall -y transformers peft\n\n# 2. C√†i ƒë·∫∑t phi√™n b·∫£n t∆∞∆°ng th√≠ch (Transformers 4.44.2 v√† PEFT m·ªõi nh·∫•t)\n!pip install -q transformers==4.44.2\n!pip install -q peft==0.12.0\n!pip install -q huggingface_hub librosa\n\nimport os\nos.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'\nprint(\"‚úÖ ƒê√£ c√†i ƒë·∫∑t Transformers 4.44.2 & PEFT 0.12.0 (T∆∞∆°ng th√≠ch ho√†n to√†n).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T09:03:53.387358Z","iopub.execute_input":"2025-11-18T09:03:53.387542Z","iopub.status.idle":"2025-11-18T09:05:28.203064Z","shell.execute_reply.started":"2025-11-18T09:03:53.387527Z","shell.execute_reply":"2025-11-18T09:05:28.202212Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.53.3\nUninstalling transformers-4.53.3:\n  Successfully uninstalled transformers-4.53.3\nFound existing installation: peft 0.16.0\nUninstalling peft-0.16.0:\n  Successfully uninstalled peft-0.16.0\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m‚úÖ ƒê√£ c√†i ƒë·∫∑t Transformers 4.44.2 & PEFT 0.12.0 (T∆∞∆°ng th√≠ch ho√†n to√†n).\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- CELL 2: IMPORT & C·∫§U H√åNH ---\nimport torch, numpy as np, pandas as pd\nimport torchaudio\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, Wav2Vec2FeatureExtractor, Wav2Vec2Model\nfrom sklearn.metrics import f1_score, accuracy_score\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\n# Import LoRA\nfrom peft import get_peft_model, LoraConfig, TaskType\n\n# Config\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"üü¢ Device: {device}\")\n\nDATA_DIR = Path(\"/kaggle/input/meld-emotion-recognition/MELD.Raw/MELD.Raw\")\nTRAIN_CSV = DATA_DIR / \"train/train_sent_emo.csv\"\nDEV_CSV = DATA_DIR / \"dev_sent_emo.csv\"\nAUDIO_TRAIN = DATA_DIR / \"train/train_splits\"\nAUDIO_DEV = DATA_DIR / \"dev/dev_splits_complete\"\n\nTEXT_MODEL = \"roberta-large\"\nAUDIO_MODEL = \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n\nEMOTIONS = ['anger','disgust','fear','joy','neutral','sadness','surprise']\nLABEL2ID = {e:i for i,e in enumerate(EMOTIONS)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T09:05:28.205134Z","iopub.execute_input":"2025-11-18T09:05:28.205383Z","iopub.status.idle":"2025-11-18T09:05:35.360382Z","shell.execute_reply.started":"2025-11-18T09:05:28.205361Z","shell.execute_reply":"2025-11-18T09:05:35.359651Z"}},"outputs":[{"name":"stdout","text":"üü¢ Device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- CELL 3: TR√çCH XU·∫§T AUDIO (GI·ªÆ NGUY√äN V√å N√ì ƒêANG T·ªêT) ---\n# Ch√∫ng ta v·∫´n ƒë√≥ng bƒÉng Audio ƒë·ªÉ ti·∫øt ki·ªám VRAM cho LoRA Text\nprint(\"üöÄ ƒêang t·∫£i Audio Model ƒë·ªÉ tr√≠ch xu·∫•t...\")\n\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(AUDIO_MODEL)\naudio_model = Wav2Vec2Model.from_pretrained(AUDIO_MODEL).to(device)\naudio_model.eval()\n\ndef extract_audio_features(csv_path, audio_dir):\n    df = pd.read_csv(csv_path)\n    features_dict = {}\n    \n    print(f\"Processing audio for {csv_path.name}...\")\n    for _, row in tqdm(df.iterrows(), total=len(df)):\n        uid = f\"dia{row['Dialogue_ID']}_utt{row['Utterance_ID']}\"\n        file_path = audio_dir / f\"{uid}.mp4\"\n        \n        default_vec = np.zeros(1024, dtype=np.float32)\n        \n        if not file_path.exists():\n            features_dict[uid] = default_vec\n            continue\n            \n        try:\n            speech, sr = torchaudio.load(file_path)\n            if sr != 16000:\n                resampler = torchaudio.transforms.Resample(sr, 16000)\n                speech = resampler(speech)\n            \n            speech = speech.squeeze()\n            if speech.ndim > 1: speech = speech.mean(dim=0)\n            if len(speech) < 160:\n                 features_dict[uid] = default_vec\n                 continue\n\n            inputs = feature_extractor(\n                speech.numpy(), \n                sampling_rate=16000, \n                return_tensors=\"pt\",\n                padding=\"max_length\",\n                max_length=16000*4, \n                truncation=True\n            )\n            input_values = inputs.input_values.to(device)\n            \n            with torch.no_grad():\n                outputs = audio_model(input_values)\n                pooled_output = torch.mean(outputs.last_hidden_state, dim=1)\n            \n            vec = pooled_output.cpu().numpy().flatten()\n            if vec.shape != (1024,): features_dict[uid] = default_vec\n            else: features_dict[uid] = vec\n            \n        except Exception as e:\n            features_dict[uid] = default_vec\n            \n    return features_dict\n\naudio_features_train = extract_audio_features(TRAIN_CSV, AUDIO_TRAIN)\naudio_features_dev = extract_audio_features(DEV_CSV, AUDIO_DEV)\n\ndel audio_model, feature_extractor\ntorch.cuda.empty_cache()\nprint(\"‚úÖ Audio extraction complete! VRAM cleared.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T09:05:35.361122Z","iopub.execute_input":"2025-11-18T09:05:35.361562Z","iopub.status.idle":"2025-11-18T09:25:43.977562Z","shell.execute_reply.started":"2025-11-18T09:05:35.361541Z","shell.execute_reply":"2025-11-18T09:25:43.976731Z"}},"outputs":[{"name":"stdout","text":"üöÄ ƒêang t·∫£i Audio Model ƒë·ªÉ tr√≠ch xu·∫•t...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc55842f77d94f28b5c37d97996bf4be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6742e1a80fac490c8f03b12627b1ef51"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d79d670ff9da40458e55f147135f3321"}},"metadata":{}},{"name":"stderr","text":"Some weights of Wav2Vec2Model were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Processing audio for train_sent_emo.csv...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/9989 [00:00<?, ?it/s]2025-11-18 09:05:43.498690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763456743.723721      39 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763456743.791776      39 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9989/9989 [18:02<00:00,  9.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing audio for dev_sent_emo.csv...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1109/1109 [01:59<00:00,  9.28it/s]","output_type":"stream"},{"name":"stdout","text":"‚úÖ Audio extraction complete! VRAM cleared.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- CELL 4: DATASET ---\nclass MultimodalDataset(Dataset):\n    def __init__(self, csv_path, audio_features_dict):\n        self.df = pd.read_csv(csv_path)\n        self.audio_features = audio_features_dict\n        self.tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL)\n        \n    def __len__(self): return len(self.df)\n    \n    def get_context_text(self, idx):\n        row = self.df.iloc[idx]\n        ctx = self.df[(self.df[\"Dialogue_ID\"]==row[\"Dialogue_ID\"]) & (self.df[\"Utterance_ID\"]<row[\"Utterance_ID\"])].tail(3)\n        context = \" </s> \".join([f\"{r['Speaker']}: {r['Utterance']}\" for _, r in ctx.iterrows()])\n        return f\"{context} </s> </s> {row['Speaker']}: {row['Utterance']}\"\n\n    def __getitem__(self, idx):\n        text = self.get_context_text(idx)\n        row = self.df.iloc[idx]\n        uid = f\"dia{row['Dialogue_ID']}_utt{row['Utterance_ID']}\"\n        \n        enc = self.tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n        audio_vec = self.audio_features.get(uid, np.zeros(1024, dtype=np.float32))\n        \n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n            \"audio_emb\": torch.tensor(audio_vec, dtype=torch.float32),\n            \"label\": torch.tensor(LABEL2ID[row[\"Emotion\"]])\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T09:25:43.978533Z","iopub.execute_input":"2025-11-18T09:25:43.978969Z","iopub.status.idle":"2025-11-18T09:25:43.986944Z","shell.execute_reply.started":"2025-11-18T09:25:43.978939Z","shell.execute_reply":"2025-11-18T09:25:43.986087Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# --- S·ª¨A L·∫†I CELL 5 ---\n\nclass LoRAMultimodalModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        print(\"Loading RoBERTa-Large...\")\n        base_model = AutoModel.from_pretrained(TEXT_MODEL)\n        \n        # --- ‚≠êÔ∏è THAY ƒê·ªîI QUAN TR·ªåNG: C·∫§U H√åNH M·∫†NH H∆†N ‚≠êÔ∏è ---\n        peft_config = LoraConfig(\n            r=32,              # TƒÉng t·ª´ 16 l√™n 32 (Th√¥ng minh h∆°n)\n            lora_alpha=64,     # Alpha th∆∞·ªùng = 2 * r\n            lora_dropout=0.1,\n            bias=\"none\",\n            task_type=TaskType.FEATURE_EXTRACTION,\n            # T·∫•n c√¥ng to√†n di·ªán: Train T·∫§T C·∫¢ c√°c l·ªõp quan tr·ªçng c·ªßa RoBERTa\n            target_modules=[\"query\", \"key\", \"value\", \"dense\"] \n        )\n        # -----------------------------------------------------\n        \n        self.roberta = get_peft_model(base_model, peft_config)\n        self.roberta.print_trainable_parameters() # B·∫°n s·∫Ω th·∫•y s·ªë tham s·ªë train tƒÉng l√™n\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(1024 + 1024, 512), \n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, len(EMOTIONS))\n        )\n        \n    def forward(self, input_ids, attention_mask, audio_emb):\n        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        text_emb = outputs.last_hidden_state[:, 0, :] \n        combined = torch.cat((text_emb, audio_emb), dim=1)\n        logits = self.classifier(combined)\n        return logits\n\nprint(\"Ki·∫øn tr√∫c LoRA (Target All Linear) ƒë√£ s·∫µn s√†ng.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:48:10.063206Z","iopub.execute_input":"2025-11-18T10:48:10.063958Z","iopub.status.idle":"2025-11-18T10:48:10.071129Z","shell.execute_reply.started":"2025-11-18T10:48:10.063931Z","shell.execute_reply":"2025-11-18T10:48:10.070232Z"}},"outputs":[{"name":"stdout","text":"Ki·∫øn tr√∫c LoRA (Target All Linear) ƒë√£ s·∫µn s√†ng.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# --- S·ª¨A L·∫†I CELL 6 ---\n\nBATCH_SIZE = 8\nACCUMULATION = 4\nEPOCHS = 15 \nLR = 1e-4   # <-- GI·∫¢M T·ª™ 2e-4 XU·ªêNG 1e-4 ƒê·ªÇ ·ªîN ƒê·ªäNH H∆†N\n\n# ... (Ph·∫ßn DataLoader gi·ªØ nguy√™n: num_workers=0) ...\ntrain_ds = MultimodalDataset(TRAIN_CSV, audio_features_train)\nval_ds = MultimodalDataset(DEV_CSV, audio_features_dev)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=0)\n\n# Init Model\nmodel = LoRAMultimodalModel().to(device)\noptimizer = AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n\n# ... (Ph·∫ßn Scheduler v√† Loop gi·ªØ nguy√™n) ...\ntotal_steps = len(train_loader) * EPOCHS // ACCUMULATION\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1*total_steps), num_training_steps=total_steps)\nloss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n\nbest_f1 = 0\nprint(f\"üöÄ B·∫Øt ƒë·∫ßu Fine-tuning RoBERTa (LoRA r=32, All-Linear) + Audio...\")\n\nfor ep in range(EPOCHS):\n    model.train()\n    train_loss = 0\n    \n    for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {ep+1}\")):\n        input_ids = batch[\"input_ids\"].to(device)\n        mask = batch[\"attention_mask\"].to(device)\n        audio_emb = batch[\"audio_emb\"].to(device)\n        labels = batch[\"label\"].to(device)\n        \n        logits = model(input_ids, mask, audio_emb)\n        loss = loss_fn(logits, labels)\n        \n        loss = loss / ACCUMULATION\n        loss.backward()\n        \n        if (i+1) % ACCUMULATION == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            \n        train_loss += loss.item() * ACCUMULATION\n        \n    # Validate\n    model.eval()\n    all_preds, all_trues = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            mask = batch[\"attention_mask\"].to(device)\n            audio_emb = batch[\"audio_emb\"].to(device)\n            \n            logits = model(input_ids, mask, audio_emb)\n            preds = torch.argmax(logits, dim=1).cpu().numpy()\n            \n            all_preds.extend(preds)\n            all_trues.extend(batch[\"label\"].numpy())\n            \n    val_f1 = f1_score(all_trues, all_preds, average=\"weighted\")\n    acc = accuracy_score(all_trues, all_preds)\n    \n    print(f\"Epoch {ep+1}: Loss={train_loss/len(train_loader):.4f} | Val F1={val_f1:.4f} | Acc={acc:.4f}\")\n    \n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        torch.save(model.state_dict(), \"best_lora_model.pt\")\n        print(f\"‚úÖ New Best F1: {best_f1:.4f} - Saved!\")\n\nprint(f\"üèÜ K·∫æT QU·∫¢ CU·ªêI C√ôNG: {best_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:48:10.291831Z","iopub.execute_input":"2025-11-18T10:48:10.292303Z","iopub.status.idle":"2025-11-18T11:58:42.804921Z","shell.execute_reply.started":"2025-11-18T10:48:10.292283Z","shell.execute_reply":"2025-11-18T11:58:42.803868Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Loading RoBERTa-Large...\ntrainable params: 14,221,312 || all params: 369,581,056 || trainable%: 3.8480\nüöÄ B·∫Øt ƒë·∫ßu Fine-tuning RoBERTa (LoRA r=32, All-Linear) + Audio...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1249/1249 [11:01<00:00,  1.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Loss=1.6348 | Val F1=0.4870 | Acc=0.5528\n‚úÖ New Best F1: 0.4870 - Saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1249/1249 [11:01<00:00,  1.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Loss=1.2729 | Val F1=0.6102 | Acc=0.6438\n‚úÖ New Best F1: 0.6102 - Saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1249/1249 [11:02<00:00,  1.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Loss=1.1528 | Val F1=0.6659 | Acc=0.6817\n‚úÖ New Best F1: 0.6659 - Saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1249/1249 [11:03<00:00,  1.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Loss=1.0732 | Val F1=0.6762 | Acc=0.6880\n‚úÖ New Best F1: 0.6762 - Saved!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1249/1249 [11:01<00:00,  1.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Loss=0.9882 | Val F1=0.6483 | Acc=0.6438\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1249/1249 [11:02<00:00,  1.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Loss=0.8982 | Val F1=0.6720 | Acc=0.6817\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7:   6%|‚ñå         | 69/1249 [00:36<10:27,  1.88it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_39/2732042511.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_39/1686751674.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, audio_emb)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtext_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}