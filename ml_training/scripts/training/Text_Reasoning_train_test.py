import json
import random
import time
import os
# ==============================================================================
# 1. Cáº¤U HÃŒNH (CONFIGURATION)
# ==============================================================================
TARGET_TOTAL = 6000       # Tá»•ng sá»‘ máº«u má»¥c tiÃªu
TRAIN_RATIO = 0.9         # Tá»· lá»‡ Train (90%), Test (10%)
OUTPUT_TRAIN = "../../data/reasoning/Text_Reasoning_train.jsonl"
OUTPUT_TEST = "../../data/reasoning/Text_Reasoning_test.jsonl"

os.makedirs("../../data/reasoning", exist_ok=True)

print(f"ğŸš€ Äang khá»Ÿi Ä‘á»™ng bá»™ sinh dá»¯ liá»‡u Super Generator...")
print(f"ğŸ¯ Má»¥c tiÃªu: {TARGET_TOTAL} dÃ²ng khÃ´ng trÃ¹ng láº·p.")
print(f"âœ‚ï¸ Cháº¿ Ä‘á»™ chia: {int(TRAIN_RATIO*100)}% Train - {int((1-TRAIN_RATIO)*100)}% Test")

# ==============================================================================
# 2. KHO Tá»ª Vá»°NG KHá»”NG Lá»’ (EXPANDED VOCABULARY)
# ==============================================================================

# --- Äáº I Tá»ª & Tá»ª Äá»†M (DÃ¹ng chung) ---
pronouns = [
    "Em", "MÃ¬nh", "Tá»›", "ChÃ¡u", "TÃ´i", "Anh", "Chá»‹", "BÃ¡c", "Con", "Tao", "Tui", 
    "NgÆ°á»i nhÃ  em", "Báº¡n em", "Vá»£ mÃ¬nh", "Chá»“ng mÃ¬nh"
]

timeframes = [
    "dáº¡o gáº§n Ä‘Ã¢y", "máº¥y hÃ´m nay", "tá»« tuáº§n trÆ°á»›c", "máº¥y thÃ¡ng nay rá»“i", "tá»« lÃºc chia tay", 
    "sau khi sinh em bÃ©", "tá»« Ä‘á»£t dá»‹ch Ä‘áº¿n giá»", "gáº§n Ä‘Ã¢y", "bá»¯a giá»", "suá»‘t 2 tuáº§n nay",
    "cáº£ nÄƒm nay", "má»›i hÃ´m qua", "tá»± nhiÃªn hÃ´m nay"
]

fillers = [
    "thá»±c sá»±", "hÃ¬nh nhÆ°", "cÃ³ váº»", "cháº¯c lÃ ", "vÃ´ cÃ¹ng", "ráº¥t chi lÃ ", "hÆ¡i bá»‹", "khÃ¡ lÃ ",
    "cáº£m tháº¥y", "tháº¥y", "tá»± dÆ°ng", "bá»—ng nhiÃªn", "cháº£ hiá»ƒu sao", "stress vÃ£i", "chÃ¡n ghÃª",
    "huhu", "haizz", "trá»i Æ¡i", "khá»• tÃ¢m ghÃª", "buá»“n thá»‘i ruá»™t"
]

# --- VOCAB CHO INFORMATIONAL (Kiáº¿n thá»©c) ---
info_concepts = [
    "tráº§m cáº£m", "rá»‘i loáº¡n lo Ã¢u", "rá»‘i loáº¡n lÆ°á»¡ng cá»±c", "tÃ¢m tháº§n phÃ¢n liá»‡t", "OCD", "PTSD",
    "ADHD á»Ÿ ngÆ°á»i lá»›n", "chá»©ng máº¥t ngá»§ mÃ£n tÃ­nh", "rá»‘i loáº¡n Äƒn uá»‘ng vÃ´ Ä‘á»™", "chÃ¡n Äƒn tÃ¢m tháº§n",
    "burnout (kiá»‡t sá»©c)", "tráº§m cáº£m sau sinh", "stress kÃ©o dÃ i", "rá»‘i loáº¡n nhÃ¢n cÃ¡ch ranh giá»›i (BPD)",
    "tá»± ká»· Ã¡m thá»‹", "chá»©ng sá»£ xÃ£ há»™i", "rá»‘i loáº¡n hoáº£ng sá»£", "rá»‘i loáº¡n cÆ¡ thá»ƒ hÃ³a", "nghiá»‡n rÆ°á»£u",
    "nghiá»‡n game", "hÃ nh vi tá»± háº¡i", "liá»‡u phÃ¡p CBT", "thuá»‘c chá»‘ng tráº§m cáº£m SSRI"
]

info_questions = [
    "lÃ  bá»‡nh gÃ¬", "Ä‘á»‹nh nghÄ©a lÃ  gÃ¬", "cÃ³ triá»‡u chá»©ng tháº¿ nÃ o", "nguyÃªn nhÃ¢n do Ä‘Ã¢u", 
    "cÃ³ chá»¯a khá»i háº³n Ä‘Æ°á»£c khÃ´ng", "cháº©n Ä‘oÃ¡n á»Ÿ Ä‘Ã¢u uy tÃ­n", "biá»ƒu hiá»‡n ra sao", 
    "khÃ¡c gÃ¬ vá»›i buá»“n bÃ¬nh thÆ°á»ng", "dÃ¹ng thuá»‘c gÃ¬ Ä‘á»ƒ chá»¯a", "theo tiÃªu chuáº©n DSM-5 lÃ  gÃ¬", 
    "theo MHGAP xá»­ lÃ½ sao", "cÃ³ di truyá»n khÃ´ng", "kÃ©o dÃ i bao lÃ¢u thÃ¬ khá»i",
    "cÃ³ nguy hiá»ƒm tÃ­nh máº¡ng khÃ´ng", "phÃ¢n loáº¡i tháº¿ nÃ o", "cÃ³ máº¥y giai Ä‘oáº¡n"
]

info_prefixes = [
    "Cho há»i", "Ad Æ¡i cho há»i", "Muá»‘n tÃ¬m hiá»ƒu vá»", "Äá»‹nh nghÄ©a cá»§a", "ThÃ´ng tin vá»",
    "LÃ m sao biáº¿t mÃ¬nh bá»‹", "PhÃ¢n biá»‡t giÃºp mÃ¬nh", "Giáº£i thÃ­ch thuáº­t ngá»¯", "TÃ¬m tÃ i liá»‡u vá»",
    "BÃ¡c sÄ© cho há»i", "Em muá»‘n há»i chÃºt vá»", "Search dÃ¹m mÃ¬nh", "Cho mÃ¬nh xin info vá»", 
    "TÃ´i cáº§n tÃ¬m hiá»ƒu", ""
]

# --- VOCAB CHO COMPLEX REASONING (TÆ° váº¥n sÃ¢u) ---
complex_contexts = [
    "Ã¡p lá»±c cÃ´ng viá»‡c quÃ¡ lá»›n", "vá»«a chia tay ngÆ°á»i yÃªu xong", "bá»‘ máº¹ ly hÃ´n", "máº¥t viá»‡c lÃ m Ä‘á»™t ngá»™t",
    "ná»£ náº§n chá»“ng cháº¥t xÃ£ há»™i Ä‘en Ä‘Ã²i", "bá»‹ Ä‘á»“ng nghiá»‡p toxic báº¯t náº¡t", "con cÃ¡i hÆ° há»ng khÃ´ng nghe lá»i", 
    "ngÆ°á»i thÃ¢n vá»«a máº¥t", "thi trÆ°á»£t Ä‘áº¡i há»c", "bá»‹ body shaming bÃ©o quÃ¡", "cáº£m tháº¥y láº¡c lÃµng trong nhÃ³m báº¡n",
    "sáº¯p pháº£i thuyáº¿t trÃ¬nh trÆ°á»›c Ä‘Ã¡m Ä‘Ã´ng", "bá»‹ sáº¿p dÃ­ deadline", "vá»«a sinh con xong stress quÃ¡",
    "gia Ä‘Ã¬nh chá»“ng kháº¯t khe", "ngÆ°á»i yÃªu vÃ´ tÃ¢m", "há»c hÃ nh sa sÃºt", "bá»‹ lá»«a Ä‘áº£o máº¥t tiá»n",
    "sá»‘ng xa nhÃ  cÃ´ Ä‘Æ¡n", "khÃ´ng cÃ³ báº¡n thÃ¢n", "bá»‹ pháº£n bá»™i", "tháº¥t báº¡i trong kinh doanh"
]

complex_symptoms = [
    "máº¥t ngá»§ triá»n miÃªn tráº¯ng Ä‘Ãªm", "Äƒn khÃ´ng ngon miá»‡ng sá»¥t cÃ¢n", "tim Ä‘áº­p nhanh khÃ³ thá»Ÿ nhÆ° sáº¯p ngáº¥t", 
    "run tay chÃ¢n báº§n báº­t", "hay khÃ³c tháº§m má»—i Ä‘Ãªm", "khÃ´ng muá»‘n gáº·p ai chá»‰ muá»‘n trá»‘n trong phÃ²ng", 
    "Ä‘áº§u Ã³c trá»‘ng rá»—ng khÃ´ng táº­p trung Ä‘Æ°á»£c", "hay cÃ¡u gáº¯t vÃ´ cá»› vá»›i ngÆ°á»i nhÃ ", 
    "máº¥t há»©ng thÃº vá»›i má»i sá»Ÿ thÃ­ch cÅ©", "luÃ´n cáº£m tháº¥y tá»™i lá»—i dáº±n váº·t", "nghÄ© ngá»£i lung tung cáº£ Ä‘Ãªm",
    "sá»£ tiáº¿ng Ä‘á»™ng lá»›n", "hay quÃªn trÆ°á»›c quÃªn sau", "Ä‘au Ä‘áº§u dá»¯ dá»™i Ä‘i khÃ¡m khÃ´ng ra bá»‡nh",
    "cáº£m giÃ¡c nhÆ° cÃ³ ai theo dÃµi", "nghe tháº¥y tiáº¿ng nÃ³i trong Ä‘áº§u", "bá»“n chá»“n khÃ´ng yÃªn"
]

complex_requests = [
    "liá»‡u cÃ³ pháº£i bá»‹ tráº§m cáº£m khÃ´ng?", "bÃ¡c sÄ© tÆ° váº¥n giÃºp vá»›i áº¡.", "lÃ m sao Ä‘á»ƒ vÆ°á»£t qua giai Ä‘oáº¡n nÃ y?",
    "cÃ³ cÃ¡ch nÃ o cÃ¢n báº±ng láº¡i cáº£m xÃºc khÃ´ng?", "tÃ´i sá»£ mÃ¬nh bá»‹ bá»‡nh tÃ¢m lÃ½ náº·ng.", 
    "tÃ´i báº¿ táº¯c quÃ¡ khÃ´ng biáº¿t lÃ m sao thoÃ¡t ra.", "cáº§n lá»i khuyÃªn gáº¥p áº¡.", 
    "lÃ m sao Ä‘á»ƒ vui váº» trá»Ÿ láº¡i nhÆ° xÆ°a?", "cÃ³ nÃªn Ä‘i khÃ¡m bÃ¡c sÄ© tÃ¢m lÃ½ khÃ´ng?",
    "em pháº£i lÃ m gÃ¬ bÃ¢y giá»?", "giÃºp em vá»›i em má»‡t má»i quÃ¡.", "cÃ³ ai tá»«ng bá»‹ nhÆ° nÃ y chÆ°a?"
]

# --- VOCAB CHO HIGH RISK (Nguy cÆ¡ cao) ---
risk_triggers = [
    "tuyá»‡t vá»ng tá»™t cÃ¹ng rá»“i", "khÃ´ng cÃ²n lá»‘i thoÃ¡t nÃ o ná»¯a", "chÃ¡n ghÃ©t báº£n thÃ¢n kinh khá»§ng", 
    "cuá»™c sá»‘ng nÃ y vÃ´ nghÄ©a toÃ n Ä‘au khá»•", "táº­n cÃ¹ng ná»—i Ä‘au rá»“i", "kiá»‡t sá»©c hoÃ n toÃ n rá»“i",
    "khÃ´ng ai thÆ°Æ¡ng mÃ¬nh cáº£", "mÃ¬nh lÃ  gÃ¡nh náº·ng cá»§a gia Ä‘Ã¬nh", "tháº¿ giá»›i nÃ y tÃ n nháº«n quÃ¡",
    "máº¥t háº¿t hy vá»ng rá»“i", "cáº£m giÃ¡c nhÆ° Ä‘Ã£ cháº¿t á»Ÿ bÃªn trong"
]

risk_actions = [
    "muá»‘n cháº¿t Ä‘i cho xong", "muá»‘n tá»± tá»­ ngay bÃ¢y giá»", "Ä‘á»‹nh uá»‘ng thuá»‘c ngá»§ Ä‘á»ƒ Ä‘i luÃ´n",
    "muá»‘n nháº£y láº§u káº¿t thÃºc táº¥t cáº£", "Ä‘ang cáº§m dao muá»‘n ráº¡ch tay cho bá»›t Ä‘au lÃ²ng", 
    "sáº½ biáº¿n máº¥t khá»i tháº¿ giá»›i nÃ y vÄ©nh viá»…n", "khÃ´ng muá»‘n nhÃ¬n tháº¥y ngÃ y mai ná»¯a", 
    "cÃ¡i cháº¿t lÃ  sá»± giáº£i thoÃ¡t duy nháº¥t", "Ä‘á»‹nh lao Ä‘áº§u vÃ o xe táº£i", "muá»‘n treo cá»• tá»± váº«n",
    "Æ°á»›c gÃ¬ ngá»§ má»™t giáº¥c khÃ´ng bao giá» dáº­y ná»¯a", "tÃ¬m cÃ¡ch káº¿t liá»…u cuá»™c Ä‘á»i"
]

risk_plans = [
    "táº¡m biá»‡t má»i ngÆ°á»i nhÃ©.", "Ä‘Ã£ viáº¿t thÆ° tuyá»‡t má»‡nh Ä‘á»ƒ láº¡i rá»“i.", "khÃ´ng ai cá»©u Ä‘Æ°á»£c tÃ´i Ä‘Ã¢u.",
    "xin lá»—i bá»‘ máº¹ con Ä‘i Ä‘Ã¢y.", "Ä‘Ãªm nay sáº½ lÃ  Ä‘Ãªm cuá»‘i cÃ¹ng.", "chá»‹u háº¿t ná»•i rá»“i bye bye.",
    "tÃ´i Ä‘i Ä‘Ã¢y Ä‘á»«ng tÃ¬m tÃ´i ná»¯a.", "Ä‘Ã£ chuáº©n bá»‹ sáºµn thuá»‘c rá»“i.", "Ä‘ang Ä‘á»©ng trÃªn cáº§u giÃ³ mÃ¡t quÃ¡.",
    "Ä‘á»«ng khuyÃªn tÃ´i ná»¯a vÃ´ Ã­ch thÃ´i."
]

# ==============================================================================
# 3. CÃC HÃ€M SINH Dá»® LIá»†U (GENERATORS)
# ==============================================================================

def gen_informational():
    # Style 1: Formal (Trang trá»ng) - 50%
    if random.random() < 0.5:
        text = f"{random.choice(info_prefixes)} {random.choice(info_concepts)} {random.choice(info_questions)}?"
    # Style 2: Short/Direct (Ngáº¯n gá»n) - 50%
    else:
        text = f"{random.choice(info_concepts)} {random.choice(info_questions)}?"
    
    # LÃ m sáº¡ch khoáº£ng tráº¯ng thá»«a
    text = " ".join(text.split()).strip()
    if not text.endswith("?"): text += "?"
    return text.capitalize()

def gen_complex_reasoning():
    pronoun = random.choice(pronouns)
    context = random.choice(complex_contexts)
    symptom = random.choice(complex_symptoms)
    request = random.choice(complex_requests)
    time = random.choice(timeframes)
    filler = random.choice(fillers)
    
    style = random.randint(1, 4)
    
    if style == 1: # Full story: Context -> Symptom -> Request
        text = f"{pronoun} bá»‹ {context}, {time} {pronoun} tháº¥y {symptom}. {request}"
    elif style == 2: # Symptom focus: Time -> Symptom -> Filler -> Context
        text = f"{time} {pronoun} tháº¥y {symptom} do {context}. {pronoun} {filler} lo láº¯ng, {request}"
    elif style == 3: # Question first: Request -> Context
        text = f"{request} {pronoun} cá»© {symptom} mÃ£i, cÃ³ pháº£i do {context} khÃ´ng?"
    else: # Conversational/Teen code (Natural noise)
        text = f"{context} khiáº¿n {pronoun} {filler}, giá» {symptom} suá»‘t. {request}"
        
    return text

def gen_high_risk():
    pronoun = random.choice(pronouns)
    trigger = random.choice(risk_triggers)
    action = random.choice(risk_actions)
    plan = random.choice(risk_plans)
    
    style = random.randint(1, 3)
    
    if style == 1: # Full explicit
        text = f"{pronoun} {trigger}, {pronoun} {action}. {plan}"
    elif style == 2: # Action focus
        text = f"{action}. {plan}"
    else: # Cry for help
        text = f"Cá»©u {pronoun} vá»›i, {pronoun} Ä‘ang nghÄ© quáº©n {action}."
        
    return text

# ==============================================================================
# 4. MAIN LOOP & SPLIT LOGIC
# ==============================================================================

def main():
    data = []
    seen_hashes = set() # DÃ¹ng hash Ä‘á»ƒ check trÃ¹ng láº·p cá»±c nhanh
    samples_per_class = TARGET_TOTAL // 3
    
    print("â³ Äang báº¯t Ä‘áº§u sinh dá»¯ liá»‡u...")

    # --- GIAI ÄOáº N 1: SINH & Lá»ŒC TRÃ™NG ---
    generators = [
        ("informational", gen_informational),
        ("complex_reasoning", gen_complex_reasoning),
        ("high_risk", gen_high_risk)
    ]

    for label, generator_func in generators:
        print(f"   ğŸ”¹ Äang sinh nhÃ³m: {label}...", end="\r")
        count = 0
        attempts = 0
        while count < samples_per_class:
            text = generator_func()
            
            # KIá»‚M TRA TRÃ™NG Láº¶P
            if text not in seen_hashes:
                data.append({"text": text, "label": label})
                seen_hashes.add(text)
                count += 1
            
            attempts += 1
            if attempts > samples_per_class * 20: # TrÃ¡nh vÃ²ng láº·p vÃ´ táº­n náº¿u háº¿t tá»«
                print(f"\nâš ï¸ Cáº£nh bÃ¡o: KhÃ´ng thá»ƒ sinh thÃªm máº«u duy nháº¥t cho {label}. Dá»«ng á»Ÿ {count}.")
                break
        print(f"   âœ… Xong nhÃ³m {label}: {count} dÃ²ng.")

    # --- GIAI ÄOáº N 2: XÃO TRá»˜N ---
    print("ğŸ”„ Äang xÃ¡o trá»™n (Shuffle) dá»¯ liá»‡u...")
    random.shuffle(data)

    # --- GIAI ÄOáº N 3: CHIA TÃCH (SPLIT) ---
    split_idx = int(len(data) * TRAIN_RATIO)
    
    train_data = data[:split_idx]
    test_data = data[split_idx:]

    # --- GIAI ÄOáº N 4: LÆ¯U FILE ---
    print(f"ğŸ’¾ Äang lÆ°u file Train ({len(train_data)} dÃ²ng)...")
    with open(OUTPUT_TRAIN, "w", encoding="utf-8") as f:
        for entry in train_data:
            json.dump(entry, f, ensure_ascii=False)
            f.write("\n")

    print(f"ğŸ’¾ Äang lÆ°u file Test ({len(test_data)} dÃ²ng)...")
    with open(OUTPUT_TEST, "w", encoding="utf-8") as f:
        for entry in test_data:
            json.dump(entry, f, ensure_ascii=False)
            f.write("\n")

    # --- Tá»”NG Káº¾T ---
    print("="*50)
    print("ğŸ‰ HOÃ€N Táº¤T QUÃ TRÃŒNH!")
    print(f"ğŸ“Š Tá»•ng sá»‘ máº«u Ä‘Ã£ sinh: {len(data)}")
    print(f"ğŸ“‚ File Train: {OUTPUT_TRAIN} ({len(train_data)} máº«u)")
    print(f"ğŸ“‚ File Test:  {OUTPUT_TEST} ({len(test_data)} máº«u)")
    print("ğŸ‘‰ BÆ°á»›c tiáº¿p theo: Upload 2 file nÃ y lÃªn Google Colab Ä‘á»ƒ Train model.")
    print("="*50)

if __name__ == "__main__":
    main()